---
layout: post
comments: true
title:  "2014~2017 的一些项目"
date:   2023-3-11 21:14:54
categories: 毕业十年
tags: 毕业十年
typora-root-url: ../../halfcoffee
---

* content
{:toc}
# 2014~2017 的一些项目



## 某城商行核心 N7K 引擎扩容项目

这个项目的背景可能有些匪夷所思，因为一般核心网络设计一定是各种冗余，竟然还会有设备单引擎的设计？

后来打听了下，原来项目规划的是双机单引擎板，有了双机，单引擎这种额外的冗余显得就没太大必要，但是有次银行做灾备演练时发现竟然没切成功，造成业务中断。返回来查时发现某些汇聚是单线接入到单个核心，并不是像理想的状态双线接到两个核心...

这个问题一时半会儿没办法解决，只能给所有核心设备增加引擎，提升单机的可靠性...

这个项目的难度还是有些高，主要问题在于新购买的引擎软件版本低，已有设备软件版本高，还得先去升级新引擎软件版本，再做扩容，而 Nexus 这种百万级的核心设备基本找不到备件来做降级以及测试验证。于是实施前花了很长时间啃文档，完善实施流程。另外割接时间只有 2 小时，在银行的灾备演练之前进行，如果我的工作完不成，会直接影响后面的灾备演练，一堆人可能都会白来现场。

即使准备的万全，最终还是翻车了，碰到两个坑：

第一个坑是给引擎升级完软件后，发现系统竟然报内存不足，后来看到原来引擎配置了 8G 内存，而新的引擎配置了 4G，新版本的 NXOS 需要 8G 才能跑起来。之前设备采购谁都没想到这个问题...

另一个坑是在做引擎切换时，原来的引擎竟然挂了，起不来了，后来根据日志判断是内存坏掉了，最后没办法只能把新引擎的两个内存条都拔下来换上，最终进行了回退。

后来和公司前辈反馈了问题，根据经验判断是某个年份采购的思科设备均有内存问题，断电重启后内存会挂，后来在一个运营商的项目也碰到了，几年后我和思科的人聊起往事，他也承认了这个事实。

有了前车之鉴，公司 PM 立马加急采购内存，为了防止意外发生，额外采购了 2 条 4G 内存 ，最终证明该来的还会再来，第二次割接时另一台 N7K 的内存也挂掉了...

这个项目工作量并没有多少，但是我一个人独立完成的第一个项目，还记得割接前在监控室几十个人在等待，他们能否早早回家取决于我是否顺利割接，也记得第二天凌晨，出门的时候牛肉面馆已开门，吃个早饭安心的睡了。有辛苦，也有收获、认可和满足。

## 某园区新建项目

介入这个项目时，实际上实施已经有一段时间了，一来项目缺人，二来碰到虚拟化的一些疑难杂症，项目上亟需外援。

开车和 PM、总监、其他小朋友一起到了现场，第一件事就是解燃眉之急，为啥同一批服务器，同一批网络设备，有些服务器网络通，有些不通，找不到任何规律。

已经忘记的排查思路是什么样的，只记得检查过软件配置没问题，然后去测交换机直连服务器时，发现网络通了...

之后一一测试发现，只要直连网络就能通，进而推测是配线架搞的鬼，后来丢给总包，找师傅重做配线架去了。

在这个 Case 里，会发现真实世界的排错并不是独立掌握好一门技术就好，曾经试用期学的虚拟化帮到了我，之前各种实验对我的排错思维敏感度提升也有一些帮助，或者换句话说，知识广和知识深同等重要。

在这之外，项目里基本涵盖了常见的各种思科设备，N7k 当核心，C45xx 做汇聚，无线全家桶，一个项目下来收获不少。

项目里还有一件事让我耿耿于怀，当时总包 PM 是个黝黑的小伙子，很年轻，一开始的时候很是佩服他的胆识，直到他当着我的面，开始质疑我的技术...

也不知道是不是出于年轻人的冲动，我打心底里不想再支持他的项目，后来迟迟没给他一些测试报告，结果闹得不咋愉快。

就像《头脑特工队》里的场景一样，不愉快的记忆有时候反倒印象深刻，如果当时的我不在乎那些评价，或许是另一种结果。

## 某科研院所园区新建项目

刚入司的第一个目标是学习 Cisco 安全，所学知识在这个项目里就用到了，所有安全设备的安装实施由我来负责，核心设备的安装也有我来，过程非常顺利，直到后来，同事重做了我安装的核心设备...

这就需要谈到理论和实践这两个词。

不得不佩服 Cisco 是家伟大的公司，产品设计领先，官方文档也非常好，即使没有什么资源，也可以从官方文档里学到很多知识，和”最佳实践“。

网络的核心是实现端到端连通性，为了保证连通性通常需要各种冗余，从设备内到设备间，随着网络演进出现了一大堆高可用相关的技术，VSS 就是其中一种，后来很多交换机或者其他设备都具备类似 VSS 一样的软堆叠方案。

这种方案在文档中描述很全面，无论是功能还是防脑裂都有详细的文档记录，因此在项目之初我想都没想用了这种架构，这种架构还有个好处就是可以使用 LACP 等链路捆绑技术，链路冗余也可以做的很好。

之后大概一个月听说现场工程师把 VSS 拆了，换成了传统的 STP+VRRP 架构。

有次在公司技术分享会上，谈及这部分的设计，我举手问为什么不使用 VSS，根据文档看能够最大程度提供高可用，保证故障发生后快速切换。技术总监听到这个问题，耐心做了解答，大致记得他说：”经过多年的实践，STP+VRRP 是最可靠稳定的，而 VSS 会碰到切换时间长的问题，造成网络长时间不能恢复“。

后来简单分析过背后的逻辑：STP+VRRP 下始终是两台大脑在同时独立工作；而 VSS 下是单个大脑，另一台即使是热备，大脑切换也需要时间。

这就是理论和实践的差距。往往一个传统的技术，即使配置可能会复杂，也可能有些限制，但是它就是很稳。

多年后我一直记得这个事，几年后还真碰到过 H3C 设备 IRF 脑裂问题，或者设备重启丢配置等问题，没有大脑的设备恢复起来也是异常麻烦...



## 某公司的无线网扩容及改造

这是我做的第一个也是唯一一个无线项目，伴随着无线项目，还有一些遗留的 PIX 防火墙、ASA、专线。

这个客户应该是 2015 年开始接触的，最早是在他们办公区卖上网行为管理设备，第一次接触这类设备，当时对于国产设备能实现的功能感到惊叹，能上什么网站、不能上什么网站都能进行控制，设备能够识别的协议也很丰富。也是这个项目，让我接触到了老早已经淘汰的 PIX 防火墙，曾经只是在 CCIE 的培训中接触过 PIX，因为当时模拟器还只能模拟 PIX 防火墙，所以多少熟悉一些，项目整体顺利，只是在未正式上线前出过一次小故障（可能和当天另一个变更有关），导致所有用户无法上网，为了这事还专门去总部和领导汇报，当时并不觉得这是多大的事。

回到无线项目，公司在 2015 年的思科网络项目还算比较多，不过来来去去也就交换、安全、无线三类设备，公司有闲置的 WLC（无线控制器）和 AP，于是早早地玩了下，还测试过 ACS 对接之类的，所学基本都在这个项目里用到了，为了这个项目，其实也没少加班去看文档，做验证。

项目本身不是特别复杂，但从没人这样做过，大概背景如下：
客户有三个园区，之前有两个地方布置了无线，现在想在第三个地方也安装无线。原来的系统都是独立的，使用本地认证，所以这次项目除了扩容无线，也想做一下统一管理，实现三个地方的用户漫游。

这里面牵扯到三类系统，AD/DNS，ACS 和 WLC，三个东西加一起有一定难度，但如果有环境多做实验，学起来也快：

- AD/DNS：公司在本地并没有 AD，所有用户都在总部的 AD，所以此处只是去申请 AD 权限，为无线用户新建 OU，把相应的用户加进来
- ACS：在新大楼部署 HA ACS，为三个中心提供统一 AAA 服务
- WLC：新大楼部署 HA WLC，其他两个地方部署单节点 WLC，但是所有 WLC 在一个组，所以当单节点设备故障时，AP 可以连接到其他 WLC，保证无线网络始终可访问



这个项目最终比较顺利，一个项目下来和客户关系也比较好，个人认为还是技术上得到了客户认可，后续两年有很多零碎的事基本都是我来处理，包括 PIX 换 ASA、多线路路由调整、二期无线系统优化、ACS VM 迁移、某业务故障转移集群部署等。某种程度上来说，我成了客户的运维。

这些零碎的事并非全无价值，可以看到涉及的面比较多，也基本和我知识的转型匹配。

这里很多事情也比较轻松，基本在下午下班后都可以进行，很少熬夜，工作时间也不长，有些不影响网络的变更在上午或者中午就做了，下午可以提早回家，所以一直以来也比较喜欢跑这个客户。

不过不好的地方就是收益不大，每年靠维保和零碎的买一些设备，还投入这么多人力，在公司层面其实算是亏的，不过这也不是我关心的，领导没去想这些，我更没理由去想。



差不多在 2016 年，我们对一期的无线项目做了改造，原因还和之前的设计相关，我们发现虽然 WLC 之类的都有高可用，但 ACS 只在一个中心，当中心间网络阻塞的时候，某些地方的无线认证会失败，所以最终还是在每个地方都部署了 ACS，多组 ACS 间做配置同步。同时也对广域网路由做了调整，保证三个中心间全部是双线（MSTP 专线+IPsec VPN），这里面其实已经牵扯到最早的 SDWAN 解决的痛点，多中心加多链路的管理痛点。



## 某单位核心网改造及机房搬迁

这个用户是我第一份工作花时间最长的用户，一切都起始于他们的机房搬迁，伴随着机房搬迁，有一笔新的费用想把之前的思科设备更换为华三。

整个项目最难的有两点：1、设备间兼容性，2、不断网割接

项目细节现在基本忘记了，为了这个项目，在现场丢失一副新买的降噪耳机，为此心疼了很久。

客户有两个部门，一个系统部一个网络部，但总共加起来也就 3 个人，这个项目是网络部门主导，



## 某公司 VPN 改造

这可能是我工作期间碰到最难啃的骨头，因为结局是花了两个周末，和销售说放弃了这个项目....



具体项目细节忘得差不多了，不过发现 2018 年竟然非常生动的写过这个故事，链接在此：

[原来四年前就做过混合云项目，只是当时太年轻](https://mp.weixin.qq.com/s?__biz=MzUxODgwOTkyMQ==&mid=2247484462&idx=1&sn=f716eb965ca31d515d13ef802d57b0a4&chksm=f9827400cef5fd168cc3f6c6278f95336b6e5d20b3fb6d3ca5e4824b0755cb221f9e6bad0203&token=1684466491&lang=zh_CN#rd)



## Horizon View POC

因为试用期时学完了 VMware 虚拟化，而当时因为工程师变化（由独立的项目部转为工程师下放到每个业务部），部门里没有其他多余的人来做 VMware 的项目，于是只能由我上，摸石头过河。

在 2015 年一年内，给三个客户做了 Horizon View 的 POC，第一个算是上手，熟悉基本的组件架构，第二个则是做 GPU 虚拟化+ RDSH App，第三个是 GPU+VDI。

在当时 GPU 虚拟化出来时间不长，公司拿到了 NVIDIA Grid K1 及 K2 卡专门用于验证，Horizon 支持 RDSH 及 vGPU 的时间也不是很长，在区域内基本没多少人用，拿着原厂写的手册一步步摸索着，反复做各种测试。

其实对于熟悉电脑装机的我来说，这些搭建起来没有什么难度，服务器也就只是个大号的 PC 而已，RDSH 也就是个远程桌面而已，vGPU 也就是一个 GPU 给多个用户去切片使用，在当时还只能做到内存的切割，CUDA 核心本身还是所有用户共享。这些东西都很基本，理解清楚里面的关联关系很快就能搭好，反而 VDI 里最难得是优化性能，同时不降低用户体验。

TBC
