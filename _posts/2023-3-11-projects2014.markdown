---
layout: post
comments: true
title:  "2014~2017 的一些项目"
date:   2023-3-14 21:14:54
categories: 毕业十年
tags: 毕业十年
typora-root-url: ../../halfcoffee
---

* content
{:toc}
# 2014~2017 的一些项目



## 某城商行核心 N7K 引擎扩容项目

这个项目的背景可能有些匪夷所思，因为一般核心网络设计一定是各种冗余，竟然还会有设备单引擎的设计？

后来打听了下，原来项目规划的是双机单引擎板，有了双机，单引擎这种额外的冗余显得就没太大必要，但是有次银行做灾备演练时发现竟然没切成功，造成业务中断。返回来查时发现某些汇聚是单线接入到单个核心，并不是像理想的状态双线接到两个核心...

这个问题一时半会儿没办法解决，只能给所有核心设备增加引擎，提升单机的可靠性...

这个项目的难度还是有些高，主要问题在于新购买的引擎软件版本低，已有设备软件版本高，还得先去升级新引擎软件版本，再做扩容，而 Nexus 这种百万级的核心设备基本找不到备件来做降级以及测试验证。于是实施前花了很长时间啃文档，完善实施流程。另外割接时间只有 2 小时，在银行的灾备演练之前进行，如果我的工作完不成，会直接影响后面的灾备演练，一堆人可能都会白来现场。

即使准备的万全，最终还是翻车了，碰到两个坑：

第一个坑是给引擎升级完软件后，发现系统竟然报内存不足，后来看到原来引擎配置了 8G 内存，而新的引擎配置了 4G，新版本的 NXOS 需要 8G 才能跑起来。之前设备采购谁都没想到这个问题...

另一个坑是在做引擎切换时，原来的引擎竟然挂了，起不来了，后来根据日志判断是内存坏掉了，最后没办法只能把新引擎的两个内存条都拔下来换上，最终进行了回退。

后来和公司前辈反馈了问题，根据经验判断是某个年份采购的思科设备均有内存问题，断电重启后内存会挂，后来在一个运营商的项目也碰到了，几年后我和思科的人聊起往事，他也承认了这个事实。

有了前车之鉴，公司 PM 立马加急采购内存，为了防止意外发生，额外采购了 2 条 4G 内存 ，最终证明该来的还会再来，第二次割接时另一台 N7K 的内存也挂掉了...

这个项目工作量并没有多少，但是我一个人独立完成的第一个项目，还记得割接前在监控室几十个人在等待，他们能否早早回家取决于我是否顺利割接，也记得第二天凌晨，出门的时候牛肉面馆已开门，吃个早饭安心的睡了。有辛苦，也有收获、认可和满足。

## 某园区新建项目

介入这个项目时，实际上实施已经有一段时间了，一来项目缺人，二来碰到虚拟化的一些疑难杂症，项目上亟需外援。

开车和 PM、总监、其他小朋友一起到了现场，第一件事就是解燃眉之急，为啥同一批服务器，同一批网络设备，有些服务器网络通，有些不通，找不到任何规律。

已经忘记的排查思路是什么样的，只记得检查过软件配置没问题，然后去测交换机直连服务器时，发现网络通了...

之后一一测试发现，只要直连网络就能通，进而推测是配线架搞的鬼，后来丢给总包，找师傅重做配线架去了。

在这个 Case 里，会发现真实世界的排错并不是独立掌握好一门技术就好，曾经试用期学的虚拟化帮到了我，之前各种实验对我的排错思维敏感度提升也有一些帮助，或者换句话说，知识广和知识深同等重要。

在这之外，项目里基本涵盖了常见的各种思科设备，N7k 当核心，C45xx 做汇聚，无线全家桶，一个项目下来收获不少。

项目里还有一件事让我耿耿于怀，当时总包 PM 是个黝黑的小伙子，很年轻，一开始的时候很是佩服他的胆识，直到他当着我的面，开始质疑我的技术...

也不知道是不是出于年轻人的冲动，我打心底里不想再支持他的项目，后来迟迟没给他一些测试报告，结果闹得不咋愉快。

就像《头脑特工队》里的场景一样，不愉快的记忆有时候反倒印象深刻，如果当时的我不在乎那些评价，或许是另一种结果。

## 某科研院所园区新建项目

刚入司的第一个目标是学习 Cisco 安全，所学知识在这个项目里就用到了，所有安全设备的安装实施由我来负责，核心设备的安装也有我来，过程非常顺利，直到后来，同事重做了我安装的核心设备...

这就需要谈到理论和实践这两个词。

不得不佩服 Cisco 是家伟大的公司，产品设计领先，官方文档也非常好，即使没有什么资源，也可以从官方文档里学到很多知识，和”最佳实践“。

网络的核心是实现端到端连通性，为了保证连通性通常需要各种冗余，从设备内到设备间，随着网络演进出现了一大堆高可用相关的技术，VSS 就是其中一种，后来很多交换机或者其他设备都具备类似 VSS 一样的软堆叠方案。

这种方案在文档中描述很全面，无论是功能还是防脑裂都有详细的文档记录，因此在项目之初我想都没想用了这种架构，这种架构还有个好处就是可以使用 LACP 等链路捆绑技术，链路冗余也可以做的很好。

之后大概一个月听说现场工程师把 VSS 拆了，换成了传统的 STP+VRRP 架构。

有次在公司技术分享会上，谈及这部分的设计，我举手问为什么不使用 VSS，根据文档看能够最大程度提供高可用，保证故障发生后快速切换。技术总监听到这个问题，耐心做了解答，大致记得他说：”经过多年的实践，STP+VRRP 是最可靠稳定的，而 VSS 会碰到切换时间长的问题，造成网络长时间不能恢复“。

后来简单分析过背后的逻辑：STP+VRRP 下始终是两台大脑在同时独立工作；而 VSS 下是单个大脑，另一台即使是热备，大脑切换也需要时间。

这就是理论和实践的差距。往往一个传统的技术，即使配置可能会复杂，也可能有些限制，但是它就是很稳。

多年后我一直记得这个事，几年后还真碰到过 H3C 设备 IRF 脑裂问题，或者设备重启丢配置等问题，没有大脑的设备恢复起来也是异常麻烦...



## 某公司的无线网扩容及改造

这是我做的第一个也是唯一一个无线项目，伴随着无线项目，还有一些遗留的 PIX 防火墙、ASA、专线。

这个客户应该是 2015 年开始接触的，最早是在他们办公区卖上网行为管理设备，第一次接触这类设备，当时对于国产设备能实现的功能感到惊叹，能上什么网站、不能上什么网站都能进行控制，设备能够识别的协议也很丰富。也是这个项目，让我接触到了老早已经淘汰的 PIX 防火墙，曾经只是在 CCIE 的培训中接触过 PIX，因为当时模拟器还只能模拟 PIX 防火墙，所以多少熟悉一些，项目整体顺利，只是在未正式上线前出过一次小故障（可能和当天另一个变更有关），导致所有用户无法上网，为了这事还专门去总部和领导汇报，当时并不觉得这是多大的事。

回到无线项目，公司在 2015 年的思科网络项目还算比较多，不过来来去去也就交换、安全、无线三类设备，公司有闲置的 WLC（无线控制器）和 AP，于是早早地玩了下，还测试过 ACS 对接之类的，所学基本都在这个项目里用到了，为了这个项目，其实也没少加班去看文档，做验证。

项目本身不是特别复杂，但从没人这样做过，大概背景如下：
客户有三个园区，之前有两个地方布置了无线，现在想在第三个地方也安装无线。原来的系统都是独立的，使用本地认证，所以这次项目除了扩容无线，也想做一下统一管理，实现三个地方的用户漫游。

这里面牵扯到三类系统，AD/DNS，ACS 和 WLC，三个东西加一起有一定难度，但如果有环境多做实验，学起来也快：

- AD/DNS：公司在本地并没有 AD，所有用户都在总部的 AD，所以此处只是去申请 AD 权限，为无线用户新建 OU，把相应的用户加进来
- ACS：在新大楼部署 HA ACS，为三个中心提供统一 AAA 服务
- WLC：新大楼部署 HA WLC，其他两个地方部署单节点 WLC，但是所有 WLC 在一个组，所以当单节点设备故障时，AP 可以连接到其他 WLC，保证无线网络始终可访问



这个项目最终比较顺利，一个项目下来和客户关系也比较好，个人认为还是技术上得到了客户认可，后续两年有很多零碎的事基本都是我来处理，包括 PIX 换 ASA、多线路路由调整、二期无线系统优化、ACS VM 迁移、某业务故障转移集群部署等。某种程度上来说，我成了客户的运维。

这些零碎的事并非全无价值，可以看到涉及的面比较多，也基本和我知识的转型匹配。

这里很多事情也比较轻松，基本在下午下班后都可以进行，很少熬夜，工作时间也不长，有些不影响网络的变更在上午或者中午就做了，下午可以提早回家，所以一直以来也比较喜欢跑这个客户。

不过不好的地方就是收益不大，每年靠维保和零碎的买一些设备，还投入这么多人力，在公司层面其实算是亏的，不过这也不是我关心的，领导没去想这些，我更没理由去想。



差不多在 2016 年，我们对一期的无线项目做了改造，原因还和之前的设计相关，我们发现虽然 WLC 之类的都有高可用，但 ACS 只在一个中心，当中心间网络阻塞的时候，某些地方的无线认证会失败，所以最终还是在每个地方都部署了 ACS，多组 ACS 间做配置同步。同时也对广域网路由做了调整，保证三个中心间全部是双线（MSTP 专线+IPsec VPN），这里面其实已经牵扯到最早的 SDWAN 解决的痛点，多中心加多链路的管理痛点。



## 某单位核心网改造及机房搬迁

这个用户是我第一份工作花时间最多的用户，一切都起始于他们的机房搬迁，伴随着机房搬迁，有一笔新的费用想把之前大楼的思科设备更换为华三。

客户有两个部门，一个系统部一个网络部，但总共加起来也就 3 个人，这个项目是网络部门主导。

项目有两个难点：1、设备间兼容性，2、尽可能不断网割接

最终定下来的方案是先在新机房安装新的核心，将原有核心的配置进行翻译后配置到新核心（包括路由、Zone 防火墙等），然后跳线连接到原有的核心，接着逐步进行每个楼层设备的更换，将其连接到新的核心，最后再做网关的割接。

整个项目大概持续了三个月左右，中间很多时间花在排错，比如配置缺了、网线年代久远，达不到千兆、网络因为各种原因不通、思科和华三设备间生成树有问题等。

期间有个大楼的项目还返工，原因是距离超过 500 米，但实施方没注意用了多模光纤，最后只能重新布线，加购单模模块。

总的来说项目算顺利，在一个已有的环境中做改造，必然会碰到各种奇怪的问题。

这个项目唯一的损失，是在现场丢失了一副刚买的 Bose 降噪耳机（二手的），为此心疼了很久。不过耳机的保护袋最后被我拿来做卡包/钱包，到现在也已经用了快 10 年，虽然仿真皮有些破损，但照常在用。



紧接着网络改造的项目，便是虚拟化的迁移以及改造项目，这个项目对我之后的发展至关重要。

公司领导比较关注这个项目，想做一些创新，刚好资金也允许购买一部分的 VMware 原厂服务，来统筹新数据中心的设计。

VMware 原厂请到了前员工 T，T 是我刚入职时另一个小伙伴的堂哥（惠普过来的那位），可以说是他让我间接学了 VMware 虚拟化，刚好这个项目的客户是我负责，我也有一些虚拟化基础，于是顺理成章的，和他一起来做项目。

这个项目中，T 并不像往常一样会全程参与做设计和实施，而是更想借这个项目来培训前东家的技术团队，所以项目的很多文档都是由我来写，他进行培训和指导，最后也由我来实施（部分产品除外）。这一切都是值得的，经过这个项目，我很快了解了业界最新的技术（网络虚拟化 NSX）以及最优的技术架构。

记得这期间有几件小事：

1、写完设计文档后给甲方汇报，结果因为之前很少公开发言，开始很紧张，于是 T 接过去自己讲，T 非常善于表达，逻辑也很清晰，那个汇报我学到了很多，也知道了自己与原厂之间的差距；

2、牵扯到虚拟化网络相关设计时，T 发现当前的架构并不是他预期中的（这与我对数据中心 Spine-Leaf 架构理解不深有关系），于是他拿出几只水彩笔给我讲解理想中的架构，之后我很快就理解了他的意思，快速去机房配好了交换机。自这之后，我也继承了他用彩笔画图的习惯，做项目或者讲解东西真的很高效；

3、项目中客户购买了新的存储交换机，需要我们来实施，因此前从未碰过这类设备，于是请来外援（专门搞系统的同事），他来了之后发现项目中的设备他也没碰过，只能两个人一起查文档，摸索着配置，最后还是顺利配好了，总结就是这玩意比路由交换简单太多；

4、当时每天基本 9:30 左右到客户现场，中午去楼下德克士吃个套餐，来杯咖啡（偶尔去远处的 KFC），然后回办公室趴桌子上午休一会儿，晚上下班后等高峰期过后，再去隔壁街吃个快餐（最喜欢蒸菜），然后坐着公交晃悠着回家。一段时间后，甲方信息部副总发现总能看到我在办公室，有次见我领导时还连连夸奖，说我对项目很重视，一直恪守现场。



这个项目对我来说是比较轻松的一段时间，早期繁忙的实施过后，基本只是零碎的故障处理、做一些优化、写一些文档之类的，我也在这段时间学习了之前不熟悉的产品（云管理平台）。



在 2016 年一期上线后，和我一同负责这个客户的售前了解到一个新商机，客户有个大数据计算平台要上，需要新的计算和存储资源，刚好赶上 VMware vSAN 6.0 发布没多久，这种分布式存储天生具备大容量、高性能和可扩展的特性，很适合客户的这个应用，于是我和售前一拍即合，他负责借硬件，我负责实施，很快搭建好一个平台交付给客户测试。

开始测试的第一个月很顺利，结果第 30 多天突然出了故障，分布式存储的一个节点故障了，故障原因不明，当时并没有正式下单，于是通过关系找人排查了下，发现与硬件兼容性有关，这时我们才发现，vSAN 对于硬件有着很高的兼容性要求，HDD 和 SSD 的固件和驱动都得是指定的版本，更别说型号...而我们用的刚好是不知名厂商的 nvme...

好在第一次故障后节点重启竟然就没事了，虽然不兼容，但，又不是不能用。

这次我们也长了个教训，既然一时半会儿借不到兼容的设备，那不妨给当前的测试系统做个备份，刚好在同时期，veeam 火起来，也来我们公司宣讲过，于是二话不说，给客户部署了起来。



20 余天之后，果不其然，故障再次发生，而且比上次严重，在单节点故障后，第二个节点也发生故障，三节点集群彻底崩溃，回天无术，只能通过两天前的备份进行恢复（数据量实在太大，一次备份得耗费十几个小时），最后虽然丢了一些数据，但好在业务系统不用重部，业务侧表示当前实际已经进入预生产，很难再容忍故障，于是只能督促销售团队，快速完成软硬件采购，正式交付了这个项目。

还记得最终上服务器的时候，拿到了 Intel 数据中心级 nvme SSD 卡，一张就一万多元，真的感觉是手捧金子，小心翼翼。



从 2015 到 2017，相继在这个客户的项目里学习了 VMware SDDC 三件套 vSphere、vSAN 和 NSX，还学习了 vRA 及 UP 云管平台，这对我日后转型专做 VMware 意义重大。



在 2017 年快离职的时候，我把这个项目中所有的材料做了汇总，形成了一系列标准文档交给了同事。



## 某公司 VPN 改造

这可能是我工作期间碰到最难啃的骨头，因为结局是花了两个周末，和销售说放弃了这个项目....



具体项目细节忘得差不多了，不过发现 2018 年竟然非常生动的写过这个故事，链接在此：

[原来四年前就做过混合云项目，只是当时太年轻](https://mp.weixin.qq.com/s?__biz=MzUxODgwOTkyMQ==&mid=2247484462&idx=1&sn=f716eb965ca31d515d13ef802d57b0a4&chksm=f9827400cef5fd168cc3f6c6278f95336b6e5d20b3fb6d3ca5e4824b0755cb221f9e6bad0203&token=1684466491&lang=zh_CN#rd)



## Horizon View POC

因为试用期时学完了 VMware 虚拟化，而当时因为工程师变化（由独立的项目部转为工程师下放到每个业务部），部门里没有其他多余的人来做 VMware 的项目，于是只能由我上，摸石头过河。

在 2015 年一年内，给三个客户做了 Horizon View 的 POC，第一个算是上手，熟悉基本的组件架构，第二个则是做 GPU 虚拟化+ RDSH App，第三个是 GPU+VDI。

在当时 GPU 虚拟化出来时间不长，公司拿到了 NVIDIA Grid K1 及 K2 卡专门用于验证，Horizon 支持 RDSH 及 vGPU 的时间也不是很长，在区域内基本没多少人用，拿着原厂写的手册一步步摸索着，反复做各种测试。

其实对于熟悉电脑装机的我来说，这些搭建起来没有什么难度，服务器也就只是个大号的 PC 而已，RDSH 也就是个远程桌面而已，vGPU 也就是一个 GPU 给多个用户去切片使用，在当时还只能做到内存的切割，CUDA 核心本身还是所有用户共享。这些东西都很基本，理解清楚里面的关联关系很快就能搭好，反而 VDI 里最难得是优化性能，同时不降低用户体验。

TBC
